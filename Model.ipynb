{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Face Recogonation System</h1>\n",
    "\n",
    "The `pickle` module is used for serializing and deserializing Python objects, allowing you to save models, data, or other Python objects to disk and load them later.\n",
    "\n",
    "`NumPy` is a library for numerical operations in Python. Here, `np` is a common alias for NumPy, and it provides powerful tools for handling arrays, mathematical functions, and random number generation.\n",
    "\n",
    "`pandas` is a data manipulation library commonly used for handling and analyzing structured data in Python. `pd` is a common alias for pandas, and it provides structures like DataFrames and Series for data organization.\n",
    "\n",
    "`seaborn` is a visualization library built on top of `matplotlib`, used for making statistical graphics and visualizations in Python. `sns` is a common alias for seaborn, and it offers tools for creating attractive and informative data visualizations.\n",
    "\n",
    "`TensorFlow` is an open-source machine learning framework developed by Google. `tf` is a common alias for TensorFlow, and it provides tools for building and training deep learning models.\n",
    "\n",
    "The `to_categorical` function, part of TensorFlow’s Keras utilities, converts class vectors (integers) to binary class matrices. It is commonly used in categorical classification to convert labels into a one-hot encoded format for neural networks.\n",
    "\n",
    "`Sequential` is a linear stack of layers in Keras. It allows you to build a model by adding layers in a step-by-step sequence, making it ideal for straightforward architectures.\n",
    "\n",
    "The layers from `tensorflow.keras.layers` include:\n",
    "- `MaxPooling2D`, which applies a max-pooling operation to reduce the spatial dimensions of the input. This operation helps decrease computational requirements and mitigate overfitting.\n",
    "- `Conv2D`, a 2D convolution layer that processes 2D data, like images. This layer is commonly used in Convolutional Neural Networks (CNNs).\n",
    "- `Dense`, a fully connected layer typically used after convolution and pooling layers for classification tasks.\n",
    "- `Flatten`, which reshapes the input from 2D to 1D, enabling it to be passed to a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sv4Tm3EhQpmt"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D,Conv2D,Dense,Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines define the file paths for `X_categories.pickle` and `y_labels.pickle` as `x_path` and `y_path`, respectively. \n",
    "\n",
    "- **`r` (Raw String Prefix)**: Adding `r` before the path makes it a *raw string*, so Python treats backslashes as literal characters. This avoids issues with special characters like `\\n` or `\\t`.\n",
    "- **File Paths**: \n",
    "  - `x_path` points to the file `X_categories.pickle`, which likely contains features or input data (often `X` in machine learning).\n",
    "  - `y_path` points to the file `y_labels.pickle`, which probably contains labels or target values (commonly denoted as `y`).\n",
    "\n",
    "These files may be pickled Python objects, which can be loaded using the `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WjOkMPFTQpmv"
   },
   "outputs": [],
   "source": [
    "x_path = r'/content/X_categories.pickle'\n",
    "y_path = r'/content/y_labels.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `with open` block opens the file at `x_path` in read-binary (`'rb'`) mode. It assigns the unpickled data from the file to `X_data`. The data in this file likely represents features or input data for a machine learning model.\n",
    "\n",
    "The second `with open` block opens the file at `y_path` in read-binary (`'rb'`) mode. It assigns the unpickled data from the file to `Y_data`. This data typically represents labels or target values for model training.\n",
    "\n",
    "The `with` statement ensures that the file is properly closed after reading, which is good practice when working with files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hkPBTL1jQpmv"
   },
   "outputs": [],
   "source": [
    "with open(x_path,'rb') as file:\n",
    "    X_data = pickle.load(file)\n",
    "\n",
    "with open(y_path,'rb') as file:\n",
    "    Y_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mapping Dictionary**: The dictionary `mappings` maps numerical labels (0 to 5) to character names such as `'Donald'`, `'Mickey'`, and so on. Each number corresponds to a specific character name.\n",
    "  \n",
    "- **Convert Labels to Series**: `Y_data_count` creates a pandas `Series` from `Y_data`, which is likely a list or array of numeric labels.\n",
    "  \n",
    "- **Mapping Labels to Names**: The `map` function applies the `mappings` dictionary to replace numeric labels in `Y_data_count` with their corresponding character names. This mapped series is saved as `img_count`.\n",
    "\n",
    "- **Count Plot**: `sns.countplot(img_count)` generates a count plot with `img_count`, showing the frequency of each character label. This visualization provides insights into the distribution of labels, which is helpful for checking class balance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "423A1-klQpmv",
    "outputId": "1f244244-7593-4c70-fa35-d3fca90cd5d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mappings = {0:'Donald',1:'Mickey',2:'Minion',3:'Olaf',4:'Pooh',5:'Pumba'}\n",
    "Y_data_count = pd.Series(Y_data)\n",
    "img_count = Y_data_count.map(mappings)\n",
    "sns.countplot(img_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output the dimensions of the first item in `X_data`. Here are possible interpretations based on the typical structure of image data:\n",
    "\n",
    "- If `X_data[0]` is an **image** in RGB format, the output might be `(height, width, 3)`, where `3` represents the RGB color channels.\n",
    "- If it’s a **grayscale image**, the output might be `(height, width, 1)`, where `1` represents the single grayscale channel.\n",
    "- For **non-image data**, it could be a single-dimensional array like `(features,)`, indicating the number of features in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9yuKsKfQpmw",
    "outputId": "bf2c47c9-dec0-4164-dc0e-d4d63a30600d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression `X_data.shape[0]` returns the **number of samples** or data points in `X_data`.\n",
    "\n",
    "- **For images**: If `X_data` is an array of images, `X_data.shape[0]` gives the total number of images in the dataset.\n",
    "- **For other data types**: If `X_data` is a multi-dimensional array or matrix, `shape[0]` gives the number of rows (or samples) in the dataset.\n",
    "\n",
    "This is useful for understanding the size of your dataset, especially when performing operations like splitting data into training and test sets or analyzing dataset dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZqt4va3Qpmw",
    "outputId": "a2a6e716-61dc-4fcd-a8e2-a47964b3aca2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2453"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reshapes the `X_data` array into a new shape for use in training a neural network, especially a Convolutional Neural Network (CNN) that processes image data. Here's an explanation:\n",
    "\n",
    "1. **`X_data.shape[0]`**: This represents the number of samples (or images) in your dataset. The first dimension is preserved, meaning the reshaped array will have the same number of samples as `X_data`.\n",
    "\n",
    "2. **`224, 224`**: These are the new height and width dimensions of the images. It means that the images will be resized to 224 pixels by 224 pixels. This is a common input size for models like ResNet, VGG, and other pre-trained models in deep learning. If the images in `X_data` are not already 224x224, they will need to be resized before this reshape.\n",
    "\n",
    "3. **`3`**: This represents the number of color channels (i.e., RGB channels) for each image. This means that each pixel in the image will have three values corresponding to Red, Green, and Blue channels.\n",
    "\n",
    "- **Assumptions**: \n",
    "  - The original images in `X_data` should be of the same size or can be resized to 224x224 pixels.\n",
    "  - The images are in RGB format, meaning they have 3 channels.\n",
    "\n",
    "This reshaping prepares the data to be fed into a neural network that expects images with specific dimensions (224x224x3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3qjADRhkQpmw"
   },
   "outputs": [],
   "source": [
    "X_train = X_data.reshape(X_data.shape[0],224,224,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`to_categorical(Y_data)`**:\n",
    "   - `Y_data` is likely a list or array containing integer labels (e.g., `[0, 1, 2, 3, 4, 5]`), which represent different classes (such as 'Donald', 'Mickey', etc.).\n",
    "   - The `to_categorical` function from Keras converts these integer labels into **one-hot encoded vectors**. This is a common preprocessing step in classification tasks.\n",
    "   \n",
    "2. **`num_classes=6`**:\n",
    "   - The `num_classes=6` argument specifies that there are 6 distinct classes in your dataset. \n",
    "   - For each label in `Y_data`, it will be converted into a vector of length 6, where the position corresponding to the class label will have a value of `1`, and all other positions will have a value of `0`.\n",
    "\n",
    "This transformation is important for categorical classification tasks where the model is expected to predict one class out of a fixed number of categories (in this case, 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KgQGgQKgQpmx"
   },
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_data,num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **`Conv2D(50, (2,2), activation='relu', input_shape=(224,224,3))`**:\n",
    "   - **`Conv2D`**: This is a 2D convolutional layer that performs a convolution operation on the input image.\n",
    "   - **`50`**: The number of filters (kernels) used for the convolution. Each filter detects different features in the image, such as edges or textures.\n",
    "   - **`(2,2)`**: The size of the convolutional kernel (filter). In this case, it is a 2x2 filter.\n",
    "   - **`activation='relu'`**: The activation function used here is Rectified Linear Unit (ReLU), which introduces non-linearity to the model and helps with training.\n",
    "   - **`input_shape=(224, 224, 3)`**: This specifies the input shape of the images, which are 224x224 pixels with 3 color channels (RGB). This input shape is required for the first layer in a Sequential model.\n",
    "\n",
    "2. **`MaxPooling2D(2,2)`**:\n",
    "   - **`MaxPooling2D`**: This is a pooling layer that reduces the spatial dimensions (height and width) of the feature maps. Pooling helps reduce the computational cost and the risk of overfitting.\n",
    "   - **`(2,2)`**: This indicates a 2x2 pooling window, which means it will take the maximum value in each 2x2 region of the input.\n",
    "\n",
    "3. **`Conv2D(50, (2,2), activation='relu')`**:\n",
    "   - This is another convolutional layer with the same number of filters (50) and kernel size (2x2), applying ReLU activation. This layer is used to learn more complex features from the previous layer’s output.\n",
    "\n",
    "4. **`MaxPooling2D(2,2)`**:\n",
    "   - Similar to the previous max-pooling layer, this reduces the size of the feature maps by taking the maximum value in each 2x2 region.\n",
    "\n",
    "5. **`Flatten()`**:\n",
    "   - The `Flatten` layer converts the 2D matrix output from the previous layer into a 1D vector. This is necessary to connect the convolutional/pooling layers with fully connected (Dense) layers.\n",
    "\n",
    "6. **`Dense(20, activation='relu')`**:\n",
    "   - **`Dense`**: This is a fully connected layer. It connects every neuron in the previous layer to every neuron in this layer.\n",
    "   - **`20`**: The number of neurons in this layer. \n",
    "   - **`activation='relu'`**: Again, ReLU is used as the activation function to introduce non-linearity.\n",
    "\n",
    "7. **`Dense(40, activation='relu')`**:\n",
    "   - Another fully connected layer with 40 neurons and ReLU activation. This layer helps learn higher-level abstractions of the data.\n",
    "\n",
    "8. **`Dense(6, activation='softmax')`**:\n",
    "   - The final `Dense` layer is the output layer.\n",
    "   - **`6`**: The number of neurons corresponds to the number of classes in the classification problem (6 classes in your case).\n",
    "   - **`activation='softmax'`**: Softmax is used here because it is a multi-class classification problem. It converts the raw outputs (logits) into probabilities, where the sum of the output values will be 1, and each value represents the likelihood of a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DijI_0qlQpmx"
   },
   "outputs": [],
   "source": [
    "cnn = Sequential([Conv2D(50,(2,2),activation='relu',input_shape=(224,224,3)),\n",
    "                  MaxPooling2D(2,2),\n",
    "                  Conv2D(50,(2,2),activation='relu',input_shape=(224,224,3)),\n",
    "                  MaxPooling2D(2,2),\n",
    "                  Flatten(),\n",
    "                  Dense(20,activation='relu'),\n",
    "                  Dense(40,activation='relu'),\n",
    "                  Dense(6,activation='softmax')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`Optimizer`**: **`Adam`** - An adaptive optimizer that adjusts the learning rate during training for efficient performance.\n",
    "- **`Loss`**: **`Categorical cross-entropy`** - Used for multi-class classification with one-hot encoded labels.\n",
    "- **`Metrics`**: **`Accuracy`**, **`precision`**, and **`recall`** - Metrics to track the model's performance:\n",
    "  - **`Accuracy`**: Overall correctness of the model.\n",
    "  - **`Precision`**: Measures how many predicted positives are actually positive.\n",
    "  - **`Recall`**: Measures how many actual positives are correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "x2rvjRJvQpmx"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=['accuracy','precision', 'recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`cnn.fit()`**: This function is used to train the CNN model on the provided data.\n",
    "- **`X_train`**: The training data (images), reshaped and preprocessed, which will be fed into the model.\n",
    "- **`Y_train`**: The one-hot encoded labels corresponding to the images in `X_train`.\n",
    "- **`epochs=10`**: The model will be trained for 10 epochs, meaning the entire dataset will be passed through the network 10 times during training.\n",
    "\n",
    "This line starts the training process, adjusting the model's weights over 10 iterations (epochs) based on the training data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlcGE5u5Qpmx",
    "outputId": "53aca3c5-7fc8-4ede-b975-e0d21dd8a592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.8918 - loss: 0.3617 - precision: 0.8962 - recall: 0.8909\n",
      "Epoch 2/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.9160 - loss: 0.3251 - precision: 0.9192 - recall: 0.9156\n",
      "Epoch 3/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9036 - loss: 0.3337 - precision: 0.9098 - recall: 0.8990\n",
      "Epoch 4/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9195 - loss: 0.3037 - precision: 0.9254 - recall: 0.9151\n",
      "Epoch 5/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9131 - loss: 0.3022 - precision: 0.9161 - recall: 0.9114\n",
      "Epoch 6/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.9289 - loss: 0.2689 - precision: 0.9311 - recall: 0.9265\n",
      "Epoch 7/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9426 - loss: 0.2395 - precision: 0.9474 - recall: 0.9426\n",
      "Epoch 8/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.9370 - loss: 0.2376 - precision: 0.9405 - recall: 0.9345\n",
      "Epoch 9/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9255 - loss: 0.2745 - precision: 0.9266 - recall: 0.9216\n",
      "Epoch 10/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.9221 - loss: 0.3191 - precision: 0.9252 - recall: 0.9206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7bcaf1504070>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(X_train, Y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "BX44w-5JY_2E",
    "outputId": "464ea9b4-1df2-4934-c660-1834cfa8466e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">223</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">223</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,050</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151250</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,025,020</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">840</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">246</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m223\u001b[0m, \u001b[38;5;34m223\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │             \u001b[38;5;34m650\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m10,050\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151250\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │       \u001b[38;5;34m3,025,020\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)                  │             \u001b[38;5;34m840\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m246\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,110,420</span> (34.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,110,420\u001b[0m (34.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,036,806</span> (11.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,036,806\u001b[0m (11.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,073,614</span> (23.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m6,073,614\u001b[0m (23.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`a = X_data[1].reshape(1, 224, 224, 3)`**:\n",
    "   - This reshapes the second image in `X_data` (i.e., `X_data[1]`) to have the shape `(1, 224, 224, 3)`.\n",
    "   - The `1` indicates a single image (as the model expects a batch of images, even if it's just one). \n",
    "   - The `224x224` is the image size, and `3` refers to the three color channels (RGB).\n",
    "\n",
    "2. **`prediction = cnn.predict(a)`**:\n",
    "   - This runs the reshaped image `a` through the trained CNN model to make a prediction. \n",
    "   - The output `prediction` will be an array of class probabilities (size 6 in your case, for each class).\n",
    "\n",
    "3. **`mappings[np.argmax(prediction)]`**:\n",
    "   - `np.argmax(prediction)` finds the index of the class with the highest probability in the `prediction` array.\n",
    "   - `mappings[np.argmax(prediction)]` then maps this index to the corresponding class name (e.g., 'Donald', 'Mickey', etc.) using the `mappings` dictionary.\n",
    "\n",
    "This code reshapes a single image from `X_data`, makes a prediction using the CNN model, and then maps the predicted class index to its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "18eR9xoQQpmx",
    "outputId": "b9ddf873-eee6-4701-fa70-fde7271f9439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 803ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Pooh'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = X_data[1].reshape(1,224,224,3)\n",
    "prediction = cnn.predict(a)\n",
    "mappings[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`cnn.save('model.keras')`**: This function saves the trained CNN model to a file.\n",
    "  - **`'model.keras'`**: The file name where the model will be saved. The `.keras` extension is used to save the model in Keras's native format, which includes the architecture, weights, and training configuration.\n",
    "\n",
    "This line saves the trained model so that it can be reloaded and used later without needing to retrain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jl_2l_NcQpmx"
   },
   "outputs": [],
   "source": [
    "cnn.save('model.keras')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
